{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explainability\n",
    "Explainability is the extent to which a model can be explained in human terms. Interpretability is the extent to which you can explain the outcome of a model after a change in input parameters. Often times we are working with black box models, which only show the prediction and not the steps that led up to that decision. Explainability methods uncover vulnerabilities in models and are able to offer a broader insight as to how differing inputs change the outcome. \n",
    "\n",
    "We'll start by loading the data and splitting the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import pickle\n",
    "from alibi.explainers import CEM, KernelShap\n",
    "\n",
    "pt_info_clean = pd.read_csv(\"../data/processed/pt_info_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(pt_info_clean, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll translate the pandas data frame into numpy arrays in order to agree with the necessary inputs for our explainability methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(train.iloc[:,2:train.shape[1]])\n",
    "x_test = np.asarray(test.iloc[:,2:train.shape[1]])\n",
    "y_train = np.asarray(train['mrsa_positive'])\n",
    "y_test = np.asarray(test['mrsa_positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import our logistic regression model from before, fit it to the data, and generate predictions. Note: we could use either model here since the methods are model agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to unpickle estimator LogisticRegression from version 0.23.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "# loading model\n",
    "filename = '../models/logistic_model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# generate predictions\n",
    "y_preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has now made a classification prediction for each of the test data points. However, we don't have much intuition as to how the model chose to classify these values. Was it based off of the amount of words in each email? Number of punctuation points? Explainability methods exist to offer ways to explore the decision making process of black-box models such as this one. Let's see if we can figure out why this prediction was made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_names(idx):\n",
    "    if idx > 0:\n",
    "        print('MRSA+')\n",
    "    else:\n",
    "        print('MRSA-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Explanation Methods\n",
    "There's a variety of explainability methods that supply local or global explanations for how a model is making certain classification decisions; we'll use a library called [Alibi](https://github.com/SeldonIO/alibi) in order to create some of these methods. [Contrastive explanation methods](https://arxiv.org/abs/1802.07623), or CEMs, focus on explaining instances in terms of pertinent positives and pertinent negatives. **Pertinent positives** refer to features that should be minimally and sufficiently present to predict the same class as on the original instance. (e.g. if all people who contract MRSA are between the ages 35 and 55). **Pertinent negatives**, on the other hand, identify what features should be minimally and necessarily absent from the instance to be explained in order to maintain the original prediction class (e.g. NO people with respiratory distress contracted MRSA). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: model.predict_proba(x) # input needs the prediction \n",
    "                                              # probabilites for each class\n",
    "shape_ = (1,) + x_train.shape[1:] # dimension of one row, in this case, one patient\n",
    "feature_names = list(test.columns[2:]) \n",
    "mode = 'PN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alibi offers black-box explanations for models, which means all that the CEM needs is a predict function for the model. This gives us the flexibility to input nearly any model without having to rewrite any code to initialize the CEM. The only edit we have done is change ```predict``` to ```predict_proba```, which gives the output of the probability of each possible predicted class rather than the prediction itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/app-root/lib64/python3.8/site-packages/alibi/utils/tf.py:26: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7f37110cd496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cem = CEM(predict_fn, \n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# either PN or PP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           shape = shape_) # instance shape\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.8/site-packages/alibi/explainers/cem.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, predict, mode, shape, kappa, beta, feature_range, gamma, ae_model, learning_rate_init, max_iterations, c_init, c_steps, eps, clip, update_num_grad, no_info_val, write_dir, sess)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# define placeholders that will be assigned to relevant variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'assign_orig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'assign_adv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_adv_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'assign_adv_s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   3174\u001b[0m   \"\"\"\n\u001b[1;32m   3175\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3176\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   3177\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   3178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "cem = CEM(predict_fn, \n",
    "          mode = mode, # either PN or PP\n",
    "          shape = shape_) # instance shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "X = x_test[idx].reshape((1,) + x_test[idx].shape)\n",
    "\n",
    "cem.fit(x_train, no_info_type='median')  # we need to define what feature values contain the least\n",
    "                                         # info wrt predictions\n",
    "                                         # here we will naively assume that the feature-wise median\n",
    "                                         # contains no info\n",
    "cem_explanation = cem.explain(X, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at one singular patient and why they were classified the way they were. This is called looking at a _local explanation_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual class: ')\n",
    "class_names(y_test[idx])\n",
    "print('Model prediction for patient: ')\n",
    "class_names(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.Series(feature_names)\n",
    "pn_df = pd.DataFrame(cem_explanation.PN, \n",
    "                    columns = columns)\n",
    "pn_df.loc[:, (pn_df < 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP\n",
    "One type of explainability method is [SHAP](https://christophm.github.io/interpretable-ml-book/shap.html), or SHapley Additive exPlanations, where a local prediction is explained by displaying each feature's  contribution to the prediction. The output of a SHAP method is a linear model created for a particular instance. We'll use the [Alibi](https://github.com/SeldonIO/alibi) library again to use [KernelSHAP](https://docs.seldon.io/projects/alibi/en/latest/methods/KernelSHAP.html), which is used as a black-box SHAP method for an arbitrary classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = KernelShap(predict_fn)\n",
    "shap_explainer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explanation = shap_explainer.explain(x_test, l1_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "After initializing the explainers, we can look at both global and local explanation methods. We'll use a `force_plot` visualization to better understand the local linear model generated by SHAP. This plot shows which features contributed to making the prediction, and to what extent they moved the prediction from the base value (output of model if no inputs are given) to the output value for that instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "idx = 0\n",
    "instance = test.iloc[idx,2:test.shape[1]] # shape of the instance to be explained \n",
    "class_idx = y_preds[idx].astype(int) # predicted class\n",
    "feature_names = list(test.columns[2:]) \n",
    "\n",
    "shap.force_plot(\n",
    "    shap_explainer.expected_value[class_idx],\n",
    "    shap_explanation.shap_values[class_idx][idx, :],\n",
    "    instance,\n",
    "    feature_names = feature_names,\n",
    "    matplotlib=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a local explanation. Other instances will have different weights for each feature, so we cannot generalize this output to the whole model. We can see from the plot that the ethnicity and location of admission features had the biggest influence on the model's decision to classify this instance.  We could look at other explainability methods such as [counterfactuals](https://christophm.github.io/interpretable-ml-book/counterfactual.html) to find out what changes to the input would create the correct classification. \n",
    "\n",
    "We see what influenced this instance the most, but what about the overall model? SHAP also offers global explanations, which can be viewed best by a `summary_plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_explanation.shap_values[idx], \n",
    "                  test.iloc[:,2:test.shape[1]],  \n",
    "                  feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the total sum of each feature's SHAP value for each of the instances in `x_test` with a class `0` (MRSA negative). Features with the highest impact are at the top of the plot. This model has the highest valued features being `ethnicity_HISPANIC/LATINO - PUERTO RICAN`, `gender_F`, and `ethnicity_WHITE`; with the prescence of these values, the model is more likely to predict class `0`. This outcome definitely raises questions in terms of model bias or input bias: what does the underlying population look like that ethnicity and gender are driving features of this function? We would probably expect diagnoses to be more important, but it could be that there are a vast amount of diagnoses for any one to be heavily weighted. If this model was going into production, we would probably want to take a step back and do more research to ensure our model is able to generalize well to all patients.\n",
    " \n",
    "# Conclusion\n",
    "In the end, **explainers are not built to fix problems in models, but rather expose them.** Understanding how black-box models make decisions before launching them into production helps ensure transparency and avoid unconscious bias. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
